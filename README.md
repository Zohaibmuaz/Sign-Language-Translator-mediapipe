# âœ¨ Sign Language Translator â€“ AI-Powered Real-Time Gesture Recognition  

> **Breaking barriers with AI** â€“ Translate sign language into text in real time using computer vision and deep learning.

---

## ğŸš€ Overview
The **Sign Language Translator** is a deep learning project that uses **MediaPipe** for landmark detection and a **Transformer-based neural network** for gesture classification.  
It allows you to:
- ğŸ“¸ **Collect** custom sign language gesture data  
- ğŸ§  **Train** an advanced transformer model  
- ğŸ¥ **Translate** signs into text **live** via your webcam

---

## ğŸ›  Features
- **Real-time Detection** â€“ Instant sign-to-text translation  
- **Transformer Architecture** â€“ Modern attention-based sequence modeling  
- **Custom Dataset Collection** â€“ Capture your own gestures using `collect_data.py`  
- **Beautiful Visualizations** â€“ Real-time probability bars & styled hand/pose/face landmarks  
- **Three Sample Signs Included** â€“ `hello`, `thanks`, `iloveyou`  

---

## ğŸ“‚ Project Structure
```
ğŸ“¦ Sign Language Translator
 â”£ ğŸ“œ Sign Language Translator.ipynb   # Notebook with model training pipeline
 â”£ ğŸ“œ collct_data.py                   # Script for dataset collection
 â”£ ğŸ“œ test_1000.py                      # Real-time inference & translation
 â”£ ğŸ“œ action_1000.h5                    # Pre-trained weights (optional)
 â”— ğŸ“‚ MP_Data                           # Captured gesture data
```

---

## ğŸ“¸ Data Collection (`collct_data.py`)
Run the following to start collecting gestures:
```bash
python collct_data.py
```
- **Default Actions**: `hello`, `thanks`, `iloveyou`  
- **Sequence Length**: 30 frames per gesture  
- **Camera Index**: Change `cam_index` in the script if your webcam is not `0`.

---

## ğŸ§  Model Architecture (from `Sign Language Translator.ipynb`)
- **Input**: Sequence of 1662 keypoints (pose, face, hands) over 30 frames  
- **Feature Projection**: Dense layer to embed keypoints  
- **Positional Embedding**: Learned embeddings for sequence positions  
- **Transformer Encoder Block**: Multi-Head Attention + Feed Forward + Residual + LayerNorm  
- **Classification Head**: Dense + Softmax to output gesture probabilities  

---

## ğŸ¯ Real-Time Translation (`test_1000.py`)
Run:
```bash
python test_1000.py
```
- Press **`q`** to quit the feed  
- Adjust `CAMERA_INDEX` if your webcam feed is blank  
- Requires the pre-trained `action_1000.h5` weights  

---

## ğŸ“¦ Installation
```bash
pip install opencv-python mediapipe tensorflow numpy
```

---

## ğŸŒŸ Example Output
When you sign **"hello"**, the system displays:

```
--------------------------------
Predicted: hello   âœ… (0.92)
thanks: 0.05
iloveyou: 0.03
--------------------------------
```
â€¦and overlays styled landmarks with colored probability bars in real time!

---

## ğŸ’¡ Future Improvements
- Add more sign language gestures  
- Integrate audio output for accessibility  
- Support multiple sign languages

---

## â¤ï¸ Contributing
Pull requests are welcome! Whether itâ€™s adding new signs, improving the model, or enhancing the UI â€“ your help is appreciated.

---

## ğŸ“œ License
This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.
